import jsonlines
import json
import argparse
from pathlib import Path
import os
import copy
import re
import random

def get_json_result(text):
    try:
        res = json.loads(text)
    except Exception as Argument:
        try:
            res = text.split("\n\n")[1]
            res = json.loads(res)
        except Exception as Argument2:
            # print("============= Error Extract ===========")
            # print(Argument)
            # print(Argument2)
            return ""
    try:
        assert "satisfaction" in res
        assert "modified_constraint" in res
        assert "constraint_validity" in res
        return res
    except:
        return ""

def get_score(text):
    match = re.search(r'Score: (\d+)', text)
    if match:
        return int(match.group(1))
    else:
        return -1

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="generate")
    parser.add_argument("--dataset", type=str, default= "")
    parser.add_argument("--original_file", type=str, default="data/seed_data/Long_data/4_alpaca_filter.json")
    parser.add_argument("--input_file", type=str, default="data/seed_data/Long_data/4_alpaca_constraints_check.json")
    parser.add_argument("--output_dir", type=str, default="data/seed_data/Long_data")
    parser.add_argument("--output_file", type=str, default="5_alpaca_constraints.json")
    parser.add_argument("--threshold", type=float, default=0.0)
    args = parser.parse_args()

    output_folder = Path(args.output_dir)
    output_folder.mkdir(exist_ok=True, parents=True)
    output_file = os.path.join(output_folder, args.output_file)

    with open(args.original_file, "r", encoding="utf-8") as reader:
        ori_data = json.load(reader)
        reader.close()

    with open(args.input_file, "r", encoding="utf-8") as reader:
        data = json.load(reader)
        reader.close()


    unified_data = []
    unified_instance = copy.deepcopy(ori_data[0])
    cnt = -1
    count_score = dict()
    Final_count = dict()
    ori_id = 0
    data_id = 0
    for i, vert in enumerate(data):
        assert data_id >= i, print(data_id, i, ori_id, vert["id"], ori_data[ori_id]["id"])
        if data_id != i:
            continue
        if vert["id"] != ori_data[ori_id]["id"]:
            ori_id += 1
            unified_instance["source"] = args.dataset
            unified_data.append(unified_instance)
            for key in unified_instance["Aug_instruction"]["Additional_Instruction"].keys():
                if key not in Final_count:
                    Final_count[key] = 0
                Final_count[key] += 1
            unified_instance = copy.deepcopy(ori_data[ori_id])
        
        if vert["id"] != ori_data[ori_id]["id"]:
            # Some instances do not have constraints generated by LLM (no constraints to check), so they need to be skipped.
            while data[data_id]["id"] != ori_data[ori_id]["id"]:
                ori_id += 1
            unified_instance = copy.deepcopy(ori_data[ori_id])

        # assert vert["id"] == ori_data[ori_id]["id"], print(i,vert["id"],ori_id,ori_data[ori_id]["id"])

        data_id += 1
        res = get_json_result(vert["request"]["result"]["completions"])
        if res != "" and res["satisfaction"] == "NO":
            Dflag = (int)(random.random() < args.threshold)
            if Dflag:
                continue
            if vert["Constraints_Type"] in unified_instance["Aug_instruction"]["Additional_Instruction"]:
                # print("==========DEBUG=========")
                # print(data_id, i, ori_id)
                # print("original_constraint:",unified_instance["Aug_instruction"]["Additional_Instruction"][vert["Constraints_Type"]])
                # print("modified_constraint:",res["modified_constraint"].replace("text","response",1))
                # print("constraint_validity:",res["constraint_validity"])
                # print("###")
                # print(unified_instance["messages"][1]["content"])

                if res["constraint_validity"] == "NO":
                    del unified_instance["Aug_instruction"]["Additional_Instruction"][vert["Constraints_Type"]]
                else:
                    unified_instance["Aug_instruction"]["Additional_Instruction"][vert["Constraints_Type"]] = res["modified_constraint"].replace("text","response",1)
            if vert["Constraints_Type"] not in count_score:
                count_score[vert["Constraints_Type"]] = 0
            count_score[vert["Constraints_Type"]] += 1
        

    out_file = open(output_file, "w", encoding="utf-8")
    json.dump(
        unified_data,
        out_file,
        indent=4,
    )
    out_file.close()

    count_score = sorted(count_score.items(), key=lambda item: item[1], reverse=True)
    Final_count = sorted(Final_count.items(), key=lambda item: item[1], reverse=True)

    print("total_number:", len(unified_data))
    # print("delete count_score:", count_score)
    # print("final count_score:",Final_count)

    out_file = open(os.path.join(output_folder, args.dataset+"_constraint_type.json"), "w", encoding="utf-8")
    json.dump(
        Final_count,
        out_file,
        indent=4,
    )
    out_file.close()
